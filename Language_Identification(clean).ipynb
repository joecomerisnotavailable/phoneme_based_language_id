{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification\n",
    "\n",
    "Given a short training corpus--the preamble of the UN Universal Declaration of Human Rights in six languages--we wish to construct a language model sufficient to identify the language of future documents.\n",
    "\n",
    "\n",
    "Joe Comer\n",
    "9/22/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to get rid of the easy identifiers like diacritical marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str):\n",
    "    \"\"\"Remove diacritical marks from text.\"\"\"\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return str(only_ascii)[2:-3].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm curious how different the sets of distinct characters are after the above decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "charsets = dict()\n",
    "for root, dirs, files in os.walk(\"./Language_Identification/train/\"):\n",
    "    for filename in files:\n",
    "        if filename[:4] != \"filt\":\n",
    "            charsets[filename] = set()\n",
    "            with open(\"./Language_Identification/train/\" + filename, encoding='UTF-8') as iFile:\n",
    "                for line in iFile:\n",
    "                    string = remove_accents(line)\n",
    "                    charsets[filename] = charsets[filename].union(set(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dut.txt , eng.txt {';', ':'}\n",
      "dut.txt , esper.txt {'y', ';', 'w', ':'}\n",
      "dut.txt , frn.txt {':', 'w', ';', 'z', 'k'}\n",
      "dut.txt , ger.txt {':', ';', 'x', '\\\\'}\n",
      "dut.txt , spn.txt {'k', 'w', ':'}\n",
      "eng.txt , dut.txt {'.', 'q'}\n",
      "eng.txt , esper.txt {'y', 'w', 'q'}\n",
      "eng.txt , frn.txt {'k', 'w', 'z'}\n",
      "eng.txt , ger.txt {'q', 'x', '\\\\'}\n",
      "eng.txt , spn.txt {'k', 'w'}\n",
      "esper.txt , dut.txt {'7', '1', '-', '8', ')', '9', '4', '(', '.', '2'}\n",
      "esper.txt , eng.txt {'7', '1', '-', '8', ')', '9', '4', '(', '2'}\n",
      "esper.txt , frn.txt {'7', '1', '8', ')', '9', '4', '(', '2', 'z', 'k'}\n",
      "esper.txt , ger.txt {'x', '\\\\'}\n",
      "esper.txt , spn.txt {'k', '-'}\n",
      "frn.txt , dut.txt {'-', \"'\", '.', 'q'}\n",
      "frn.txt , eng.txt {'-', \"'\"}\n",
      "frn.txt , esper.txt {'y', \"'\", 'q'}\n",
      "frn.txt , ger.txt {'q', \"'\", 'x', '\\\\'}\n",
      "frn.txt , spn.txt {'-', \"'\"}\n",
      "ger.txt , dut.txt {'7', '1', '-', ')', '4', '9', '8', '(', '.', '2'}\n",
      "ger.txt , eng.txt {'7', '1', '-', ')', '4', '9', '8', '(', '2'}\n",
      "ger.txt , esper.txt {'w', 'y'}\n",
      "ger.txt , frn.txt {'7', '1', 'w', ')', '4', '9', '8', '(', '2', 'z', 'k'}\n",
      "ger.txt , spn.txt {'k', '-', 'w'}\n",
      "spn.txt , dut.txt {'7', '1', ')', 'q', '9', '4', '8', '(', '.', '2'}\n",
      "spn.txt , eng.txt {'7', '1', ')', '4', '9', '8', '(', ';', '2'}\n",
      "spn.txt , esper.txt {'y', ';', 'q'}\n",
      "spn.txt , frn.txt {'7', '1', ')', '4', '9', '8', '(', ';', '2', 'z'}\n",
      "spn.txt , ger.txt {'q', ';', 'x', '\\\\'}\n"
     ]
    }
   ],
   "source": [
    "for set1 in charsets:\n",
    "    for set2 in charsets:\n",
    "        if set1 != set2:\n",
    "            print(set1, \",\", set2, charsets[set1]-charsets[set2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok it looks like the sets of unique characters associated to each language are pretty similar with the exception of a few consonants that appear in some languages and not others. We will want to deal with irrelevant characters like numbers and punctuation, but first, let's be sure that the decoding didn't result in any strange, unexpected characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m', 'd', 'n', '\\\\', 's', ':', 'f', 'w', 'e', 'a', 'g', 'j', 'l', 'u', 'y', 'v', 'b', 'c', ',', 'h', 'x', '0', ' ', 'p', 'i', ';', 'z', 'r', 'k', 't', 'o'}\n",
      "{'n', '\\\\', 's', 'q', 'e', 'j', 'v', 'b', 'x', '0', 'p', 'z', 'k', 'm', 'd', 'f', 'w', '.', 'g', 'a', 'u', 'y', 'c', ',', 'h', ' ', 'i', 'l', 'r', 't', 'o'}\n",
      "{'n', '\\\\', 's', ')', '9', 'e', '2', 'j', 'v', '1', 'b', 'x', '0', '8', '4', 'p', 'z', 'k', 'm', 'd', '7', '-', 'f', '(', '.', 'a', 'g', 'u', 'c', ',', 'h', ' ', 'i', 'l', 'r', 't', 'o'}\n",
      "{'m', 'd', 'n', '\\\\', 's', '-', 'f', 'q', '.', 'e', 'a', 'g', 'j', 'u', 'y', \"'\", 'v', 'b', 'c', ',', 'h', 'x', '0', ' ', 'p', 'i', 'l', 'r', 't', 'o'}\n",
      "{'n', 's', ')', '9', 'e', '2', 'j', '1', 'v', 'b', '0', '4', '8', 'p', 'z', 'k', '7', 'd', 'm', '-', 'f', 'w', '(', '.', 'g', 'a', 'u', 'y', 'c', ',', 'h', ' ', 'i', 'l', 'r', 't', 'o'}\n",
      "{'n', '\\\\', 's', ')', 'q', '9', 'e', '2', 'j', 'v', '1', 'b', 'x', '0', '4', '8', 'p', 'z', 'm', 'd', '7', 'f', '(', '.', 'a', 'g', 'u', 'y', 'c', ',', 'h', ' ', 'i', ';', 'l', 'r', 't', 'o'}\n"
     ]
    }
   ],
   "source": [
    "for chars in charsets:\n",
    "    print(charsets[chars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No crazy wildcard characters anywhere. That's good. Looks like the accent removal went somewhat smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm curious whether one can distinguish among these languages by their patterns in vowel, nasal, fricative, and plosive consonant use. Because English spelling conventions are highly variable, I'll only be able to approximately encode the difference. For example, is 'g' a plosive or a fricative \\[or silent\\]? The cases and exceptions are too numerous to encode exhaustively. If the following naive encoding doesn't give good classifications, then I may need to try to cover some of them, but first, let's see how this simple encoding performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plosives = ['p','t','k','b','d','g','c','q']\n",
    "fricatives = ['f','g','h','j','l','r','s','v','w','y','z']\n",
    "nasals = ['n','m']\n",
    "vowels = ['a','e','i','o','u']\n",
    "\n",
    "filter_dict = dict()\n",
    "for letter in plosives:\n",
    "    filter_dict[letter] = '1'\n",
    "for letter in fricatives:\n",
    "    filter_dict[letter] = '2'\n",
    "for letter in nasals:\n",
    "    filter_dict[letter] = '3'\n",
    "for letter in vowels:\n",
    "    filter_dict[letter] = '4'\n",
    "\n",
    "# Special case for x, ng, ch, ph, sh, th\n",
    "filter_dict['x'] = '12'\n",
    "filter_dict['ng'] = '3'\n",
    "filter_dict['ch'] = '2'\n",
    "filter_dict['ph'] = '2'\n",
    "filter_dict['sh'] = '2'\n",
    "filter_dict['th'] = '2'\n",
    "\n",
    "# We don't care about numbers as distinct from one another, because all the training corpus documents\n",
    "# should include the same numbers, however,\n",
    "# it may be useful to preserve the information that a character was *some* number.\n",
    "for i in range(10):\n",
    "    filter_dict[str(i)] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because of rules involving certain consonant pairs that produce a single sound, output strings from this encoding may be shorter than the input strings. Each number is meant to represent a member of a class of sound rather than a class of letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_string(string):\n",
    "    out_string = \"\"\n",
    "    i = 0\n",
    "    while i < len(string):\n",
    "        if string[i] not in filter_dict:\n",
    "            if string[i] == \" \":\n",
    "                filtered = \" \"\n",
    "            else:\n",
    "                # Non-alphanumeric characters will receive the '?' wildcard, along\n",
    "                # with any characters not seen in the training corpus.\n",
    "                # While punctuation conventions do vary from language to language,\n",
    "                # it is not the focus of this experiment.\n",
    "                filtered = \"?\"\n",
    "        elif (i < len(string) - 1) and (string[i:i+2] in filter_dict):\n",
    "            sound = string[i:i+2]\n",
    "            filtered = filter_dict[sound]\n",
    "            i += 1\n",
    "        else:\n",
    "            sound = string[i]\n",
    "            filtered = filter_dict[sound]\n",
    "        out_string += filtered\n",
    "        i += 1\n",
    "    return out_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'142143 242 000 ?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"testing this 124 ]\"\n",
    "filter_string(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"./Language_Identification/train/\"):\n",
    "    for filename in files:\n",
    "        if filename[:4] != \"filt\":\n",
    "            with open(\"./Language_Identification/train/\" + filename, 'r+', encoding='UTF-8') as iFile:\n",
    "                with open(\"./Language_Identification/train/Filtered/filtered_\" + filename, 'w+') as oFile:\n",
    "                    for line in iFile:\n",
    "                        oFile.write(filter_string(remove_accents(line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we have the filtered versions of the text. They look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434242242 14124241443 42 24343 242212124431242242442 24142341443 42 24 43242431 1423412 431 42 24 41442 431 43424434124242212 42 422 3431422 42 24 24343 243422 42 24 2443141443 42 2244143? 2421414431 14414 43 24 24221?2242442 142242421 431 14314311 242 24343 242212 2424 24242141 43 1421424424112 2242 2424 44124241 24 1432144314 42 3431431? 431 24 412431 42 4 2422143 2242 24343 14432 2422 43242 2244143 42 21442 431 142442 431 22441432243 2442 431 2431 242 1443 1241244341 42 24 2422421 4214241443 42 24 143343144124?2242442 41 42 422431442? 42 343 42 341 14 14 143142241 14 2424 24144224? 42 4 2421242421? 14 241422443 4244321 1224332 431 4112422443? 241 24343 242212 24421 14124141141 12 24 2424 42 242?2242442 41 42 422431442 14 1243414 24 14242413431 42 22443122 242414432 14124433414432?2242442 24 1441242 42 24 434141 3414432 2424 43 24 242142 2442242341 24422442 43 24314343142 24343 242212? 43 24 1423412 431 2422 42 24 24343 142243431 43 24 41442 242212 42 343 431 24343 431 2424 1414234341 14 1243414241442 12422422 431 141142 214314212 42 2424 43 242242 2244143?2242442 343142 214142 2424 1241241 243242242 14 424424? 43 14414241443242 24 434141 3414432? 24 124341443 42 434242242 2421411 242 431 4124224314 4224343 242212 431 24314343142 22441432?2242442 4 143343 431422143143 42 2424 242212 431 22441432 42 42 2422441421 4314214314 242 24 2422 24424241443 42 242 124124?342? 24242424?24 2434242 42243122?124124432 242 434242242 14124241443 42 24343 242212 42 4 143343 21431421 424244243431 242 422 1441242 431 422 3414432? 14 24 431 241 42422 4314241442 43142422 42243 42 2414412? 144143 242 14124241443 1432143122 43 3431? 2422 212424 12?1201144243 431 414141443 14 1243414 2421411 242 2424 242212 431 22441432 431 1212422422424 34424242? 34144342 431 4314234144342? 14 241424 2442 434242242 431422411424 24142341443 431 4124224314? 142 4343 24 1441242 42 343142 214142243242242 431 4343 24 1441242 42 14224142442 43142 2442 242421411443?\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Language_Identification/train/Filtered/filtered_eng.txt\") as testFile:\n",
    "    print(next(testFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigram(bigram, string):\n",
    "    \"\"\"This will actually also count unigrams.\"\"\"\n",
    "    count = start = 0\n",
    "    while True:\n",
    "        start = string.find(bigram, start) + 1\n",
    "        if start > 0:\n",
    "            count+=1\n",
    "        else:\n",
    "            return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(filename):\n",
    "    \"\"\"Train a bigram model on a text file.\n",
    "    filename: name of the text file\n",
    "    returns: probs, dataframe of conditional probabilities\n",
    "    The i,jth entry of probs is P(j|i)\"\"\"\n",
    "    def prob(i, j, string):\n",
    "        \"\"\"Calculate the probability of j given i\"\"\"\n",
    "        first_char = str(i)\n",
    "        second_char = str(j)\n",
    "        conditional_prob = (bigram_counts[first_char + second_char] + (unigram_counts[second_char]/len(string)))/(unigram_counts[first_char] + 1)\n",
    "        return conditional_prob\n",
    "    with open(\"./Language_Identification/train/Filtered/\" + filename) as iFile:\n",
    "        corpus = next(iFile) # These files are all a single line after the filtering above.\n",
    "        \n",
    "        # Add a start character and end character to ensure a single probability distribution\n",
    "        # over documents of any length.\n",
    "        corpus = \".\" + corpus + \"!\" \n",
    "        charset = [str(x) for x in set(corpus)]\n",
    "        bigrams = [x+y for x in charset for y in charset]\n",
    "        bigram_counts = dict([(big, count_bigram(big, corpus)) for big in bigrams])\n",
    "        unigram_counts = dict([(uni, count_bigram(uni, corpus)) for uni in charset])\n",
    "        probs = pd.DataFrame(columns = [[char for char in charset]])\n",
    "        for char in charset:\n",
    "            probs.loc[char] = [prob(char,j, corpus) for j in probs.columns]\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a dictionary of probability tables for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "for root, dirs, files in os.walk(\"./Language_Identification/train/\"):\n",
    "    for filename in files:\n",
    "        if filename[:4] == \"filt\":\n",
    "            models[filename[9:]] = train_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test: What is the probability of seeing a nasal consonant following a plosive consonant in English?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32732938722241062"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['eng.txt']['1']['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_id(filename, models_dict=models):\n",
    "    print(filename)\n",
    "    with open(\"./Language_Identification/test/\" + filename,\"r+\", encoding='UTF-8') as iFile:\n",
    "        corpus = \"\"\n",
    "        for line in iFile:\n",
    "            corpus += filter_string(remove_accents(line))\n",
    "        corpus = \".\" + corpus + \"!\"\n",
    "        likelihoods = dict([(model, 0) for model in models_dict])\n",
    "        for i in range(len(corpus)-1):\n",
    "            bigram = corpus[i:i+2]\n",
    "            for model in models_dict:\n",
    "                likelihoods[model] += np.log(models_dict[model][bigram[1]][bigram[0]]) # Pandas indexing is backwards from standard matrix notation\n",
    "        most_likely = max(likelihoods.items(), key=operator.itemgetter(1))[0]\n",
    "    return most_likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dut.txt\n",
      "dut.txt ger.txt\n",
      "eng.txt\n",
      "eng.txt eng.txt\n",
      "esper.txt\n",
      "esper.txt esper.txt\n",
      "frn.txt\n",
      "frn.txt frn.txt\n",
      "ger.txt\n",
      "ger.txt ger.txt\n",
      "spn.txt\n",
      "spn.txt spn.txt\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(\"./Language_Identification/test/\"):\n",
    "    for filename in files:\n",
    "        print(filename, language_id(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive approach performs pretty well. Dutch and German are qualitatively similar in their use of phonemes, so it is unsurprising that this is where the model breaks down. It is possible that we could get better results by further investigating whether our class assignments for each letter are consistent across these two languages, but I suspect that a more granular model may be required to distinguish these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
